{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5007884",
   "metadata": {},
   "source": [
    "### 1. Create a SparkSession in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025b8a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/01 22:03:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pysparkinterviewqs').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a66daaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://anjus-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pysparkinterviewqs</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc36f79f0d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0dfb6d",
   "metadata": {},
   "source": [
    "### 2. Read a CSV file into a DataFrame using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef949ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('employees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d1dca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835965be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "|       _c0|   _c1|       _c2|            _c3|   _c4|    _c5|              _c6|                 _c7|\n",
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "|First Name|Gender|Start Date|Last Login Time|Salary|Bonus %|Senior Management|                Team|\n",
      "|   Douglas|  Male|  8/6/1993|       12:42 PM| 97308|  6.945|             true|           Marketing|\n",
      "|    Thomas|  Male| 3/31/1996|        6:53 AM| 61933|   4.17|             true|                NULL|\n",
      "|     Maria|Female| 4/23/1993|       11:17 AM|130590| 11.858|            false|             Finance|\n",
      "|     Jerry|  Male|  3/4/2005|        1:00 PM|138705|   9.34|             true|             Finance|\n",
      "|     Larry|  Male| 1/24/1998|        4:47 PM|101004|  1.389|             true|     Client Services|\n",
      "|    Dennis|  Male| 4/18/1987|        1:35 AM|115163| 10.125|            false|               Legal|\n",
      "|      Ruby|Female| 8/17/1987|        4:20 PM| 65476| 10.012|             true|             Product|\n",
      "|      NULL|Female| 7/20/2015|       10:43 AM| 45906| 11.598|             NULL|             Finance|\n",
      "|    Angela|Female|11/22/2005|        6:29 AM| 95570| 18.523|             true|         Engineering|\n",
      "|   Frances|Female|  8/8/2002|        6:51 AM|139852|  7.524|             true|Business Development|\n",
      "|    Louise|Female| 8/12/1980|        9:01 AM| 63241| 15.132|             true|                NULL|\n",
      "|     Julie|Female|10/26/1997|        3:19 PM|102508| 12.637|             true|               Legal|\n",
      "|   Brandon|  Male| 12/1/1980|        1:08 AM|112807| 17.492|             true|     Human Resources|\n",
      "|      Gary|  Male| 1/27/2008|       11:40 PM|109831|  5.831|            false|               Sales|\n",
      "|  Kimberly|Female| 1/14/1999|        7:13 AM| 41426| 14.543|             true|             Finance|\n",
      "|   Lillian|Female|  6/5/2016|        6:09 AM| 59414|  1.256|            false|             Product|\n",
      "|    Jeremy|  Male| 9/21/2010|        5:56 AM| 90370|  7.369|            false|     Human Resources|\n",
      "|     Shawn|  Male| 12/7/1986|        7:45 PM|111737|  6.414|            false|             Product|\n",
      "|     Diana|Female|10/23/1981|       10:27 AM|132940| 19.082|            false|     Client Services|\n",
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58b6dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header',True).csv('employees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee47183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "|First Name|Gender|Start Date|Last Login Time|Salary|Bonus %|Senior Management|                Team|\n",
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "|   Douglas|  Male|  8/6/1993|       12:42 PM| 97308|  6.945|             true|           Marketing|\n",
      "|    Thomas|  Male| 3/31/1996|        6:53 AM| 61933|   4.17|             true|                NULL|\n",
      "|     Maria|Female| 4/23/1993|       11:17 AM|130590| 11.858|            false|             Finance|\n",
      "|     Jerry|  Male|  3/4/2005|        1:00 PM|138705|   9.34|             true|             Finance|\n",
      "|     Larry|  Male| 1/24/1998|        4:47 PM|101004|  1.389|             true|     Client Services|\n",
      "|    Dennis|  Male| 4/18/1987|        1:35 AM|115163| 10.125|            false|               Legal|\n",
      "|      Ruby|Female| 8/17/1987|        4:20 PM| 65476| 10.012|             true|             Product|\n",
      "|      NULL|Female| 7/20/2015|       10:43 AM| 45906| 11.598|             NULL|             Finance|\n",
      "|    Angela|Female|11/22/2005|        6:29 AM| 95570| 18.523|             true|         Engineering|\n",
      "|   Frances|Female|  8/8/2002|        6:51 AM|139852|  7.524|             true|Business Development|\n",
      "|    Louise|Female| 8/12/1980|        9:01 AM| 63241| 15.132|             true|                NULL|\n",
      "|     Julie|Female|10/26/1997|        3:19 PM|102508| 12.637|             true|               Legal|\n",
      "|   Brandon|  Male| 12/1/1980|        1:08 AM|112807| 17.492|             true|     Human Resources|\n",
      "|      Gary|  Male| 1/27/2008|       11:40 PM|109831|  5.831|            false|               Sales|\n",
      "|  Kimberly|Female| 1/14/1999|        7:13 AM| 41426| 14.543|             true|             Finance|\n",
      "|   Lillian|Female|  6/5/2016|        6:09 AM| 59414|  1.256|            false|             Product|\n",
      "|    Jeremy|  Male| 9/21/2010|        5:56 AM| 90370|  7.369|            false|     Human Resources|\n",
      "|     Shawn|  Male| 12/7/1986|        7:45 PM|111737|  6.414|            false|             Product|\n",
      "|     Diana|Female|10/23/1981|       10:27 AM|132940| 19.082|            false|     Client Services|\n",
      "|     Donna|Female| 7/22/2010|        3:48 AM| 81014|  1.894|            false|             Product|\n",
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "881e42c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4eef7",
   "metadata": {},
   "source": [
    "## 3. Show the schema of a DataFrame in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd4d7223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Start Date: string (nullable = true)\n",
      " |-- Last Login Time: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- Bonus %: string (nullable = true)\n",
      " |-- Senior Management: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aff96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType,BooleanType,FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f3b168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('First Name',StringType(), False),\n",
    "    StructField('Gender',StringType(), True),\n",
    "    StructField('Start Date',StringType(),False),\n",
    "    StructField('Last Login Time', StringType(), True),\n",
    "    StructField('Salary', IntegerType(), True),\n",
    "    StructField('Bonus %',FloatType(),True),\n",
    "    StructField('Senior Management', BooleanType(),True),\n",
    "    StructField('Team',StringType(),True)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b580381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "    .option('header',True)\\\n",
    "    .schema(schema)\\\n",
    "    .csv('employees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cd7d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "|First Name|Gender|Start Date|Last Login Time|Salary|Bonus %|Senior Management|                Team|\n",
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "|   Douglas|  Male|  8/6/1993|       12:42 PM| 97308|  6.945|             true|           Marketing|\n",
      "|    Thomas|  Male| 3/31/1996|        6:53 AM| 61933|   4.17|             true|                NULL|\n",
      "|     Maria|Female| 4/23/1993|       11:17 AM|130590| 11.858|            false|             Finance|\n",
      "|     Jerry|  Male|  3/4/2005|        1:00 PM|138705|   9.34|             true|             Finance|\n",
      "|     Larry|  Male| 1/24/1998|        4:47 PM|101004|  1.389|             true|     Client Services|\n",
      "|    Dennis|  Male| 4/18/1987|        1:35 AM|115163| 10.125|            false|               Legal|\n",
      "|      Ruby|Female| 8/17/1987|        4:20 PM| 65476| 10.012|             true|             Product|\n",
      "|      NULL|Female| 7/20/2015|       10:43 AM| 45906| 11.598|             NULL|             Finance|\n",
      "|    Angela|Female|11/22/2005|        6:29 AM| 95570| 18.523|             true|         Engineering|\n",
      "|   Frances|Female|  8/8/2002|        6:51 AM|139852|  7.524|             true|Business Development|\n",
      "+----------+------+----------+---------------+------+-------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d3c0b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Start Date: string (nullable = true)\n",
      " |-- Last Login Time: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Bonus %: float (nullable = true)\n",
      " |-- Senior Management: boolean (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33036430",
   "metadata": {},
   "source": [
    "## 4. Select specific columns from a DataFrame in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ff020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06319b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/09 15:32:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('spark-test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a7574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9cb90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "[\n",
    "    Row(id=1,name='anju',age=20,gender='F',dob='12-20-2000',department='hr'),\n",
    "    Row(id=2,name='liya',age=15,gender='F',dob='12-20-1990',department='hr'),\n",
    "    Row(id=3,name='hari',age=30,gender='M',dob='12-20-2005',department='tech'),\n",
    "    Row(id=4,name='hari',age=30,gender='M',dob='12-20-2005',department='tech'),\n",
    "    Row(id=5,name='chen',age=15,gender='M',dob='12-20-2005',department='tech'),\n",
    "    Row(id=6,name='chen',age=15,gender='M',dob='12-20-2005',department='business'),\n",
    "    Row(id=7,name='chen',age=15,gender='M',dob='12-20-2005',department='business'),\n",
    "    Row(id=8,name='chen',age=15,gender='M',dob='12-20-2005',department='consulting'),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2928c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+----------+----------+\n",
      "| id|name|age|gender|       dob|department|\n",
      "+---+----+---+------+----------+----------+\n",
      "|  1|anju| 20|     F|12-20-2000|        hr|\n",
      "|  2|liya| 15|     F|12-20-1990|        hr|\n",
      "|  3|hari| 30|     M|12-20-2005|      tech|\n",
      "|  4|hari| 30|     M|12-20-2005|      tech|\n",
      "|  5|chen| 15|     M|12-20-2005|      tech|\n",
      "|  6|chen| 15|     M|12-20-2005|  business|\n",
      "|  7|chen| 15|     M|12-20-2005|  business|\n",
      "|  8|chen| 15|     M|12-20-2005|consulting|\n",
      "+---+----+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640b992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|department|\n",
      "+----+----------+\n",
      "|anju|        hr|\n",
      "|liya|        hr|\n",
      "|hari|      tech|\n",
      "|hari|      tech|\n",
      "|chen|      tech|\n",
      "|chen|  business|\n",
      "|chen|  business|\n",
      "|chen|consulting|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['name','department']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7be698",
   "metadata": {},
   "source": [
    "## 5. Filter rows based on a condition in PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3195fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+----------+----------+\n",
      "| id|name|age|gender|       dob|department|\n",
      "+---+----+---+------+----------+----------+\n",
      "|  1|anju| 20|     F|12-20-2000|        hr|\n",
      "|  2|liya| 15|     F|12-20-1990|        hr|\n",
      "+---+----+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.department=='hr').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc0dd08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+----------+----------+\n",
      "| id|name|age|gender|       dob|department|\n",
      "+---+----+---+------+----------+----------+\n",
      "|  1|anju| 20|     F|12-20-2000|        hr|\n",
      "|  3|hari| 30|     M|12-20-2005|      tech|\n",
      "|  4|hari| 30|     M|12-20-2005|      tech|\n",
      "+---+----+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.age > 15).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567311ba",
   "metadata": {},
   "source": [
    "### 6. Group by a column and perform an aggregation in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c6059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+----------+----------+\n",
      "| id|name|age|gender|       dob|department|\n",
      "+---+----+---+------+----------+----------+\n",
      "|  1|anju| 20|     F|12-20-2000|        hr|\n",
      "|  2|liya| 15|     F|12-20-1990|        hr|\n",
      "|  3|hari| 30|     M|12-20-2005|      tech|\n",
      "|  4|hari| 30|     M|12-20-2005|      tech|\n",
      "|  5|chen| 15|     M|12-20-2005|      tech|\n",
      "|  6|chen| 15|     M|12-20-2005|  business|\n",
      "|  7|chen| 15|     M|12-20-2005|  business|\n",
      "|  8|chen| 15|     M|12-20-2005|consulting|\n",
      "+---+----+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df73693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|        hr|    2|\n",
      "|      tech|    3|\n",
      "|  business|    2|\n",
      "|consulting|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.department).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f73df8",
   "metadata": {},
   "source": [
    "## 7. Join two DataFrames in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3455aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a001d506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/10 15:23:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pysparkjoin').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc89addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58167f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(id=1,name='anju',age=20,gender='F',dob='12-20-2000',department='hr'),\n",
    "    Row(id=2,name='liya',age=15,gender='F',dob='12-20-1990',department='hr'),\n",
    "    Row(id=3,name='hari',age=30,gender='M',dob='12-20-2005',department='tech'),\n",
    "    Row(id=4,name='hari',age=30,gender='M',dob='12-20-2005',department='tech'),\n",
    "    Row(id=5,name='chen',age=15,gender='M',dob='12-20-2005',department='tech'),\n",
    "    Row(id=6,name='chen',age=15,gender='M',dob='12-20-2005',department='business'),\n",
    "    Row(id=7,name='chen',age=15,gender='M',dob='12-20-2005',department='business'),\n",
    "    Row(id=8,name='chen',age=15,gender='M',dob='12-20-2005',department='consulting'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a8ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+----------+----------+\n",
      "| id|name|age|gender|       dob|department|\n",
      "+---+----+---+------+----------+----------+\n",
      "|  1|anju| 20|     F|12-20-2000|        hr|\n",
      "|  2|liya| 15|     F|12-20-1990|        hr|\n",
      "|  3|hari| 30|     M|12-20-2005|      tech|\n",
      "|  4|hari| 30|     M|12-20-2005|      tech|\n",
      "|  5|chen| 15|     M|12-20-2005|      tech|\n",
      "|  6|chen| 15|     M|12-20-2005|  business|\n",
      "|  7|chen| 15|     M|12-20-2005|  business|\n",
      "|  8|chen| 15|     M|12-20-2005|consulting|\n",
      "+---+----+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9627a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "8. Rename columns in a PySpark DataFrame.\n",
    "9. Handle missing or null values in PySpark DataFrame.\n",
    "10. Create a new column derived from existing columns in PySpark DataFrame.\n",
    "11. Remove duplicate rows from a PySpark DataFrame.\n",
    "12. Sort a DataFrame based on one or multiple columns in PySpark.\n",
    "13. Perform a simple arithmetic operation on DataFrame columns in PySpark.\n",
    "14. Calculate descriptive statistics for numeric columns in PySpark.\n",
    "15. Apply user-defined functions (UDF) on PySpark DataFrame.\n",
    "16. Convert a PySpark DataFrame to Pandas DataFrame.\n",
    "17. Write a PySpark DataFrame to a CSV file.\n",
    "18. Cache or persist a PySpark DataFrame for better performance.\n",
    "19. Handle broadcast joins in PySpark.\n",
    "20. Perform window functions in PySpark (e.g., rank, row number, etc.).\n",
    "21. Handle nested structures or arrays in PySpark DataFrame.\n",
    "22. Handle time-series data in PySpark.\n",
    "23. Calculate the correlation between columns in a PySpark DataFrame.\n",
    "24. Create a pivot table in PySpark.\n",
    "25. Perform cross-tabulation (crosstab) in PySpark.\n",
    "26. Handle large-scale data using PySpark (memory management, optimizations).\n",
    "27. Handle skewed data in PySpark.\n",
    "28. Perform machine learning tasks (e.g., regression, classification) using PySpark MLlib.\n",
    "29. Optimize PySpark jobs for performance (tuning configurations, parallelism, etc.).\n",
    "30. Handle different file formats (Parquet, Avro, ORC) in PySpark.\n",
    "31.collect list and collect set\n",
    "32. count and distinct\n",
    "33. json to dataframe and process\n",
    "34. https://www.datacamp.com/blog/pyspark-interview-questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51749d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Data Processing Optimization: How would you optimize a Spark job that processes 1 TB of data daily to reduce execution time and cost?\n",
    "\n",
    "2. Handling Skewed Data: In a Spark job, one partition is taking significantly longer to process due to skewed data. How would you handle this situation?\n",
    "\n",
    "3. Streaming Data Pipeline: Describe how you would set up a real-time data pipeline using Spark Structured Streaming to process and analyze clickstream data from a website.\n",
    "\n",
    "4. Fault Tolerance: How does Spark handle node failures during a job, and what strategies would you use to ensure data processing continues smoothly?\n",
    "\n",
    "5. Data Join Strategies: You need to join two large datasets in Spark, but you encounter memory issues. What strategies would you employ to handle this?\n",
    "\n",
    "6. Checkpointing: Explain the role of checkpointing in Spark Streaming and how you would implement it in a real-time application.\n",
    "\n",
    "7. Stateful Processing: Describe a scenario where you would use stateful processing in Spark Streaming and how you would implement it.\n",
    "\n",
    "8. Performance Tuning: What are the key parameters you would tune in Spark to improve the performance of a real-time analytics application?\n",
    "\n",
    "9. Window Operations: How would you use window operations in Spark Streaming to compute rolling averages over a sliding window of events?\n",
    "\n",
    "10. Handling Late Data: In a Spark Streaming job, how would you handle late-arriving data to ensure accurate results?\n",
    "\n",
    "11. Integration with Kafka: Describe how you would integrate Spark Streaming with Apache Kafka to process real-time data streams.\n",
    "\n",
    "12. Backpressure Handling: How does Spark handle backpressure in a streaming application, and what configurations can you use to manage it?\n",
    "\n",
    "13. Data Deduplication: How would you implement data deduplication in a Spark Streaming job to ensure unique records?\n",
    "\n",
    "14. Cluster Resource Management: How would you manage cluster resources effectively to run multiple concurrent Spark jobs without contention?\n",
    "\n",
    "15. Real-Time ETL: Explain how you would design a real-time ETL pipeline using Spark to ingest, transform, and load data into a data warehouse.\n",
    "\n",
    "16. Handling Large Files: You have a hashtag#Spark job that needs to process very large files (e.g., 100 GB). How would you optimize the job to handle such files efficiently?\n",
    "\n",
    "17. Monitoring and Debugging: What tools and techniques would you use to monitor and debug a Spark job running in production?\n",
    "\n",
    "18. Delta Lake: How would you use Delta Lake with Spark to manage real-time data lakes and ensure data consistency?\n",
    "\n",
    "19. Partitioning Strategy: How you would design an effective partitioning strategy for a large dataset.\n",
    "\n",
    "20. Data Serialization: What serialization formats would you use in Spark for real-time data processing, and why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
